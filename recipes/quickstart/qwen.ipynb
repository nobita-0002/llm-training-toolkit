{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Qwen Quick Start Notebook\n", "\n", "This notebook shows how to train and infer the Qwen-7B-Chat model on a single GPU. Similarly, Qwen-1.8B-Chat, Qwen-14B-Chat can also be leveraged for the following steps. We only need to modify the corresponding `model name` and hyper-parameters. The training and inference of Qwen-72B-Chat requires higher GPU requirements and larger disk space."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Requirements\n", "- Python 3.8 and above\n", "- Pytorch 1.12 and above, 2.0 and above are recommended\n", "- CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n", "We test the training of the model on an A10 GPU (24GB)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Extra\n", "If you need to speed up, you can install  `flash-attention`. The details of the installation can be found [here](https://github.com/Dao-AILab/flash-attention)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!git clone https://github.com/Dao-AILab/flash-attention\n", "!cd flash-attention && pip install .\n", "# Below are optional. Installing them might be slow.\n", "# !pip install csrc/layer_norm\n", "# If the version of flash-attn is higher than 2.1.1, the following is not needed.\n", "# !pip install csrc/rotary"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 0: Install Package Requirements"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["!pip install transformers>=4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed modelscope"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 1: Download Model\n", "When using `transformers` in some regions, the model cannot be automatically downloaded due to network problems. We recommend using `modelscope` to download the model first, and then use `transformers` for inference."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["from modelscope import snapshot_download\n", "\n", "# Downloading model checkpoint to a local dir model_dir.\n", "model_dir = snapshot_download('Qwen/Qwen-7B-Chat', cache_dir='.', revision='master')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 2: Direct Model Inference \n", "We recommend two ways to do model inference: `modelscope` and `transformers`.\n", "\n", "#### 2.1 Model Inference with ModelScope"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecutionIndicator": {"show": true}, "tags": []}, "outputs": [], "source": ["from modelscope import AutoModelForCausalLM, AutoTokenizer\n", "from modelscope import GenerationConfig\n", "\n", "# Note: The default behavior now has injection attack prevention off.\n", "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat/\", trust_remote_code=True)\n", "\n", "# use bf16\n", "# model = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat/\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n", "# use fp16\n", "# model = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat/\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n", "# use cpu only\n", "# model = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen-7B-Chat/\", device_map=\"cpu\", trust_remote_code=True).eval()\n", "# use auto mode, automatically select precision based on the device.\n", "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat/\", device_map=\"auto\", trust_remote_code=True).eval()\n", "\n", "# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n", "# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat/\", trust_remote_code=True) # \u53ef\u6307\u5b9a\u4e0d\u540c\u7684\u751f\u6210\u957f\u5ea6\u3001top_p\u7b49\u76f8\u5173\u8d85\u53c2\n", "\n", "# \u7b2c\u4e00\u8f6e\u5bf9\u8bdd 1st dialogue turn\n", "response, history = model.chat(tokenizer, \"\u4f60\u597d\", history=None)\n", "print(response)\n", "# \u4f60\u597d\uff01\u5f88\u9ad8\u5174\u4e3a\u4f60\u63d0\u4f9b\u5e2e\u52a9\u3002\n", "\n", "# \u7b2c\u4e8c\u8f6e\u5bf9\u8bdd 2nd dialogue turn\n", "response, history = model.chat(tokenizer, \"\u7ed9\u6211\u8bb2\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\", history=history)\n", "print(response)\n", "# \u8fd9\u662f\u4e00\u4e2a\u5173\u4e8e\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\n", "# \u6545\u4e8b\u7684\u4e3b\u4eba\u516c\u53eb\u674e\u660e\uff0c\u4ed6\u6765\u81ea\u4e00\u4e2a\u666e\u901a\u7684\u5bb6\u5ead\uff0c\u7236\u6bcd\u90fd\u662f\u666e\u901a\u7684\u5de5\u4eba\u3002\u4ece\u5c0f\uff0c\u674e\u660e\u5c31\u7acb\u4e0b\u4e86\u4e00\u4e2a\u76ee\u6807\uff1a\u8981\u6210\u4e3a\u4e00\u540d\u6210\u529f\u7684\u4f01\u4e1a\u5bb6\u3002\n", "# \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e2a\u76ee\u6807\uff0c\u674e\u660e\u52e4\u594b\u5b66\u4e60\uff0c\u8003\u4e0a\u4e86\u5927\u5b66\u3002\u5728\u5927\u5b66\u671f\u95f4\uff0c\u4ed6\u79ef\u6781\u53c2\u52a0\u5404\u79cd\u521b\u4e1a\u6bd4\u8d5b\uff0c\u83b7\u5f97\u4e86\u4e0d\u5c11\u5956\u9879\u3002\u4ed6\u8fd8\u5229\u7528\u8bfe\u4f59\u65f6\u95f4\u53bb\u5b9e\u4e60\uff0c\u79ef\u7d2f\u4e86\u5b9d\u8d35\u7684\u7ecf\u9a8c\u3002\n", "# \u6bd5\u4e1a\u540e\uff0c\u674e\u660e\u51b3\u5b9a\u5f00\u59cb\u81ea\u5df1\u7684\u521b\u4e1a\u4e4b\u8def\u3002\u4ed6\u5f00\u59cb\u5bfb\u627e\u6295\u8d44\u673a\u4f1a\uff0c\u4f46\u591a\u6b21\u90fd\u88ab\u62d2\u7edd\u4e86\u3002\u7136\u800c\uff0c\u4ed6\u5e76\u6ca1\u6709\u653e\u5f03\u3002\u4ed6\u7ee7\u7eed\u52aa\u529b\uff0c\u4e0d\u65ad\u6539\u8fdb\u81ea\u5df1\u7684\u521b\u4e1a\u8ba1\u5212\uff0c\u5e76\u5bfb\u627e\u65b0\u7684\u6295\u8d44\u673a\u4f1a\u3002\n", "# \u6700\u7ec8\uff0c\u674e\u660e\u6210\u529f\u5730\u83b7\u5f97\u4e86\u4e00\u7b14\u6295\u8d44\uff0c\u5f00\u59cb\u4e86\u81ea\u5df1\u7684\u521b\u4e1a\u4e4b\u8def\u3002\u4ed6\u6210\u7acb\u4e86\u4e00\u5bb6\u79d1\u6280\u516c\u53f8\uff0c\u4e13\u6ce8\u4e8e\u5f00\u53d1\u65b0\u578b\u8f6f\u4ef6\u3002\u5728\u4ed6\u7684\u9886\u5bfc\u4e0b\uff0c\u516c\u53f8\u8fc5\u901f\u53d1\u5c55\u8d77\u6765\uff0c\u6210\u4e3a\u4e86\u4e00\u5bb6\u6210\u529f\u7684\u79d1\u6280\u4f01\u4e1a\u3002\n", "# \u674e\u660e\u7684\u6210\u529f\u5e76\u4e0d\u662f\u5076\u7136\u7684\u3002\u4ed6\u52e4\u594b\u3001\u575a\u97e7\u3001\u52c7\u4e8e\u5192\u9669\uff0c\u4e0d\u65ad\u5b66\u4e60\u548c\u6539\u8fdb\u81ea\u5df1\u3002\u4ed6\u7684\u6210\u529f\u4e5f\u8bc1\u660e\u4e86\uff0c\u53ea\u8981\u52aa\u529b\u594b\u6597\uff0c\u4efb\u4f55\u4eba\u90fd\u6709\u53ef\u80fd\u53d6\u5f97\u6210\u529f\u3002\n", "\n", "# \u7b2c\u4e09\u8f6e\u5bf9\u8bdd 3rd dialogue turn\n", "response, history = model.chat(tokenizer, \"\u7ed9\u8fd9\u4e2a\u6545\u4e8b\u8d77\u4e00\u4e2a\u6807\u9898\", history=history)\n", "print(response)\n", "# \u300a\u594b\u6597\u521b\u4e1a\uff1a\u4e00\u4e2a\u5e74\u8f7b\u4eba\u7684\u6210\u529f\u4e4b\u8def\u300b"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 2.2 Model Inference with transformers"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecutionIndicator": {"show": true}, "tags": []}, "outputs": [], "source": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n", "from transformers.generation import GenerationConfig\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat/\", trust_remote_code=True)\n", "\n", "# use bf16\n", "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat/\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n", "# use fp16\n", "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat/\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n", "# use cpu only\n", "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat/\", device_map=\"cpu\", trust_remote_code=True).eval()\n", "# use auto mode, automatically select precision based on the device.\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    \"Qwen/Qwen-7B-Chat/\",\n", "    device_map=\"auto\",\n", "    trust_remote_code=True\n", ").eval()\n", "\n", "# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n", "# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-7B-Chat/\", trust_remote_code=True)\n", "\n", "# 1st dialogue turn\n", "response, history = model.chat(tokenizer, \"\u4f60\u597d\", history=None)\n", "print(response)\n", "# \u4f60\u597d\uff01\u5f88\u9ad8\u5174\u4e3a\u4f60\u63d0\u4f9b\u5e2e\u52a9\u3002\n", "\n", "# 2nd dialogue turn\n", "response, history = model.chat(tokenizer, \"\u7ed9\u6211\u8bb2\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\", history=history)\n", "print(response)\n", "# \u8fd9\u662f\u4e00\u4e2a\u5173\u4e8e\u4e00\u4e2a\u5e74\u8f7b\u4eba\u594b\u6597\u521b\u4e1a\u6700\u7ec8\u53d6\u5f97\u6210\u529f\u7684\u6545\u4e8b\u3002\n", "# \u6545\u4e8b\u7684\u4e3b\u4eba\u516c\u53eb\u674e\u660e\uff0c\u4ed6\u6765\u81ea\u4e00\u4e2a\u666e\u901a\u7684\u5bb6\u5ead\uff0c\u7236\u6bcd\u90fd\u662f\u666e\u901a\u7684\u5de5\u4eba\u3002\u4ece\u5c0f\uff0c\u674e\u660e\u5c31\u7acb\u4e0b\u4e86\u4e00\u4e2a\u76ee\u6807\uff1a\u8981\u6210\u4e3a\u4e00\u540d\u6210\u529f\u7684\u4f01\u4e1a\u5bb6\u3002\n", "# \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e2a\u76ee\u6807\uff0c\u674e\u660e\u52e4\u594b\u5b66\u4e60\uff0c\u8003\u4e0a\u4e86\u5927\u5b66\u3002\u5728\u5927\u5b66\u671f\u95f4\uff0c\u4ed6\u79ef\u6781\u53c2\u52a0\u5404\u79cd\u521b\u4e1a\u6bd4\u8d5b\uff0c\u83b7\u5f97\u4e86\u4e0d\u5c11\u5956\u9879\u3002\u4ed6\u8fd8\u5229\u7528\u8bfe\u4f59\u65f6\u95f4\u53bb\u5b9e\u4e60\uff0c\u79ef\u7d2f\u4e86\u5b9d\u8d35\u7684\u7ecf\u9a8c\u3002\n", "# \u6bd5\u4e1a\u540e\uff0c\u674e\u660e\u51b3\u5b9a\u5f00\u59cb\u81ea\u5df1\u7684\u521b\u4e1a\u4e4b\u8def\u3002\u4ed6\u5f00\u59cb\u5bfb\u627e\u6295\u8d44\u673a\u4f1a\uff0c\u4f46\u591a\u6b21\u90fd\u88ab\u62d2\u7edd\u4e86\u3002\u7136\u800c\uff0c\u4ed6\u5e76\u6ca1\u6709\u653e\u5f03\u3002\u4ed6\u7ee7\u7eed\u52aa\u529b\uff0c\u4e0d\u65ad\u6539\u8fdb\u81ea\u5df1\u7684\u521b\u4e1a\u8ba1\u5212\uff0c\u5e76\u5bfb\u627e\u65b0\u7684\u6295\u8d44\u673a\u4f1a\u3002\n", "# \u6700\u7ec8\uff0c\u674e\u660e\u6210\u529f\u5730\u83b7\u5f97\u4e86\u4e00\u7b14\u6295\u8d44\uff0c\u5f00\u59cb\u4e86\u81ea\u5df1\u7684\u521b\u4e1a\u4e4b\u8def\u3002\u4ed6\u6210\u7acb\u4e86\u4e00\u5bb6\u79d1\u6280\u516c\u53f8\uff0c\u4e13\u6ce8\u4e8e\u5f00\u53d1\u65b0\u578b\u8f6f\u4ef6\u3002\u5728\u4ed6\u7684\u9886\u5bfc\u4e0b\uff0c\u516c\u53f8\u8fc5\u901f\u53d1\u5c55\u8d77\u6765\uff0c\u6210\u4e3a\u4e86\u4e00\u5bb6\u6210\u529f\u7684\u79d1\u6280\u4f01\u4e1a\u3002\n", "# \u674e\u660e\u7684\u6210\u529f\u5e76\u4e0d\u662f\u5076\u7136\u7684\u3002\u4ed6\u52e4\u594b\u3001\u575a\u97e7\u3001\u52c7\u4e8e\u5192\u9669\uff0c\u4e0d\u65ad\u5b66\u4e60\u548c\u6539\u8fdb\u81ea\u5df1\u3002\u4ed6\u7684\u6210\u529f\u4e5f\u8bc1\u660e\u4e86\uff0c\u53ea\u8981\u52aa\u529b\u594b\u6597\uff0c\u4efb\u4f55\u4eba\u90fd\u6709\u53ef\u80fd\u53d6\u5f97\u6210\u529f\u3002\n", "\n", "# 3rd dialogue turn\n", "response, history = model.chat(tokenizer, \"\u7ed9\u8fd9\u4e2a\u6545\u4e8b\u8d77\u4e00\u4e2a\u6807\u9898\", history=history)\n", "print(response)\n", "# \u300a\u594b\u6597\u521b\u4e1a\uff1a\u4e00\u4e2a\u5e74\u8f7b\u4eba\u7684\u6210\u529f\u4e4b\u8def\u300b"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 3: LoRA Fine-Tuning Model (Single GPU)\n", "\n", "#### 3.1 Download Example Training Data\n", "Download the data required for training; here, we provide a tiny dataset as an example. It is sampled from [Belle](https://github.com/LianjiaTech/BELLE)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["!wget https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/release/tutorials/qwen_recipes/Belle_sampled_qwen.json"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can refer to this format to prepare the dataset. Below is a simple example list with 1 sample:\n", "\n", "```json\n", "[\n", "  {\n", "    \"id\": \"identity_0\",\n", "    \"conversations\": [\n", "      {\n", "        \"from\": \"user\",\n", "        \"value\": \"\u4f60\u597d\"\n", "      },\n", "      {\n", "        \"from\": \"assistant\",\n", "        \"value\": \"\u6211\u662f\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u53eb\u901a\u4e49\u5343\u95ee\u3002\"\n", "      }\n", "    ]\n", "  }\n", "]\n", "```\n", "\n", "You can also use multi-turn conversations as the training set. Here is a simple example:\n", "\n", "```json\n", "[\n", "  {\n", "    \"id\": \"identity_0\",\n", "    \"conversations\": [\n", "      {\n", "        \"from\": \"user\",\n", "        \"value\": \"\u4f60\u597d\"\n", "      },\n", "      {\n", "        \"from\": \"assistant\",\n", "        \"value\": \"\u4f60\u597d\uff01\u6211\u662f\u4e00\u540dAI\u52a9\u624b\uff0c\u6211\u53eb\u901a\u4e49\u5343\u95ee\uff0c\u6709\u9700\u8981\u8bf7\u544a\u8bc9\u6211\u3002\"\n", "      },\n", "      {\n", "        \"from\": \"user\",\n", "        \"value\": \"\u4f60\u90fd\u80fd\u505a\u4ec0\u4e48\"\n", "      },\n", "      {\n", "        \"from\": \"assistant\",\n", "        \"value\": \"\u6211\u80fd\u505a\u5f88\u591a\u4e8b\u60c5\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u56de\u7b54\u5404\u79cd\u9886\u57df\u7684\u95ee\u9898\u3001\u63d0\u4f9b\u5b9e\u7528\u5efa\u8bae\u548c\u6307\u5bfc\u3001\u8fdb\u884c\u591a\u8f6e\u5bf9\u8bdd\u4ea4\u6d41\u3001\u6587\u672c\u751f\u6210\u7b49\u3002\"\n", "      }\n", "    ]\n", "  }\n", "]\n", "```\n", "\n", "#### 3.2 Fine-Tune the Model\n", "\n", "You can directly run the prepared training script to fine-tune the model. Remember to check `model_name_or_path`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["!python ../finetune/deepspeed/finetune.py \\\n", "    --model_name_or_path \"Qwen/Qwen-7B-Chat/\"\\\n", "    --data_path  \"Belle_sampled_qwen.json\"\\\n", "    --bf16 \\\n", "    --output_dir \"output_qwen\" \\\n", "    --num_train_epochs 5 \\\n", "    --per_device_train_batch_size 1 \\\n", "    --per_device_eval_batch_size 1 \\\n", "    --gradient_accumulation_steps 16 \\\n", "    --evaluation_strategy \"no\" \\\n", "    --save_strategy \"steps\" \\\n", "    --save_steps 1000 \\\n", "    --save_total_limit 10 \\\n", "    --learning_rate 1e-5 \\\n", "    --weight_decay 0.1 \\\n", "    --adam_beta2 0.95 \\\n", "    --warmup_ratio 0.01 \\\n", "    --lr_scheduler_type \"cosine\" \\\n", "    --logging_steps 1 \\\n", "    --report_to \"none\" \\\n", "    --model_max_length 512 \\\n", "    --gradient_checkpointing \\\n", "    --lazy_preprocess \\\n", "    --use_lora"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3 Merge Weights\n", "\n", "LoRA training only saves the adapter parameters. You can load the fine-tuned model and merge weights as shown below:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import AutoModelForCausalLM\n", "from peft import PeftModel\n", "import torch\n", "\n", "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat/\", torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n", "model = PeftModel.from_pretrained(model, \"output_qwen/\")\n", "merged_model = model.merge_and_unload()\n", "merged_model.save_pretrained(\"output_qwen_merged\", max_shard_size=\"2048MB\", safe_serialization=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The tokenizer files are not saved in the new directory in this step. You can copy the tokenizer files or use the following code:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import AutoTokenizer\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(\n", "    \"Qwen/Qwen-7B-Chat/\",\n", "    trust_remote_code=True\n", ")\n", "\n", "tokenizer.save_pretrained(\"output_qwen_merged\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.4 Test the Model\n", "\n", "After merging the weights, we can test the model as follows:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import AutoModelForCausalLM, AutoTokenizer\n", "from transformers.generation import GenerationConfig\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(\"output_qwen_merged\", trust_remote_code=True)\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    \"output_qwen_merged\",\n", "    device_map=\"auto\",\n", "    trust_remote_code=True\n", ").eval()\n", "\n", "response, history = model.chat(tokenizer, \"\u4f60\u597d\", history=None)\n", "print(response)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.13"}, "vscode": {"interpreter": {"hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"}}}, "nbformat": 4, "nbformat_minor": 4}